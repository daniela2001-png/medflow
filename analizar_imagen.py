import time
import traceback
from modelo import init_medflow_model, device

def analizar_imagen(imagen, tipo_analisis="Reporte Estructurado", idioma="EspaÃ±ol"):
    """
    Analiza una imagen mÃ©dica utilizando el modelo Med-GEMMA.

    Procesa la imagen de entrada junto con un prompt basado en el tipo de anÃ¡lisis y idioma
    seleccionados. Genera un reporte mÃ©dico estructurado o descriptivo.

    Args:
        imagen (PIL.Image.Image): La imagen mÃ©dica a analizar.
        tipo_analisis (str, optional): El tipo de anÃ¡lisis a realizar.
            Puede ser "DescripciÃ³n General", "Hallazgos PatolÃ³gicos",
            "Reporte Estructurado", o "DiagnÃ³stico Diferencial".
            Por defecto es "Reporte Estructurado".
        idioma (str, optional): El idioma en el que se generarÃ¡ el reporte.
            Puede ser "EspaÃ±ol" o "InglÃ©s". Por defecto es "EspaÃ±ol".

    Returns:
        tuple: Una tupla conteniendo:
            - str: El reporte mÃ©dico generado o un mensaje de error.
            - str: Metadatos del procesamiento (tiempo, modelo, GPU, etc.).
            - str: El estado del proceso (Completado, Error).
    """
    # Cargar el procesador y el modelo utilizando la funciÃ³n init_medflow_model
    processor, model = init_medflow_model()

    # Verificar si el modelo y procesador se cargaron correctamente
    if processor is None or model is None:
        print("ğŸ”´ No se pudo cargar el modelo MedFlow. Verifique los mensajes de error anteriores.")
    else:
        print("ğŸŸ¢ Modelo MedFlow listo para usar.")

    # Validar si se ha cargado una imagen
    if imagen is None:
        return "âŒ Por favor carga una imagen primero", "", "Error: Sin imagen"

    try:
        inicio = time.time()

        # Prompts en espaÃ±ol para los diferentes tipos de anÃ¡lisis
        prompts = {
            "DescripciÃ³n General": "Describe esta imagen mÃ©dica identificando las estructuras anatÃ³micas visibles.",
            "Hallazgos PatolÃ³gicos": "Identifica cualquier hallazgo patolÃ³gico o anormal en esta imagen mÃ©dica.",
            "Reporte Estructurado": "Genera un reporte mÃ©dico estructurado con: TÃ‰CNICA, HALLAZGOS e IMPRESIÃ“N.",
            "DiagnÃ³stico Diferencial": "Proporciona un diagnÃ³stico diferencial basado en los hallazgos visibles."
        }

        # Obtener el prompt adecuado segÃºn el tipo de anÃ¡lisis, con fallback a "Reporte Estructurado"
        prompt = prompts.get(tipo_analisis, prompts["Reporte Estructurado"])

        # Preparar los mensajes en el formato de chat para el modelo
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "Eres un radiÃ³logo experto especializado en interpretaciÃ³n de imÃ¡genes mÃ©dicas."}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image", "image": imagen}
                ]
            }
        ]

        # Aplicar el template de chat y tokenizar la entrada
        # Primero aplicar template sin tokenizar para obtener el texto completo
        text_inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=False  # Importante: primero sin tokenizar
        )

        # Luego tokenizar por separado incluyendo la imagen
        inputs = processor(
            text=text_inputs,
            images=imagen,
            return_tensors="pt",
            padding=True
        )

        # Mover los tensores de entrada al dispositivo de procesamiento (GPU o CPU)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Obtener la longitud de los tokens de entrada para decodificar solo la respuesta
        input_len = inputs["input_ids"].shape[-1]

        # Generar la respuesta del modelo
        print(f"ğŸ¤– Generando reporte...")

        # Usar torch.no_grad() para deshabilitar el cÃ¡lculo de gradientes durante la inferencia
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=350,  # NÃºmero mÃ¡ximo de tokens a generar
                do_sample=False,  # Deshabilitar muestreo para una salida determinÃ­stica
                num_beams=1, # Usar beam search con 1 haz (equivalente a greedy search)
                pad_token_id=processor.tokenizer.pad_token_id, # ID del token de padding
                eos_token_id=processor.tokenizer.eos_token_id # ID del token de fin de secuencia
            )

        # Decodificar solo los tokens generados por el modelo (excluyendo los tokens de entrada)
        generated_tokens = outputs[0][input_len:]
        reporte = processor.decode(generated_tokens, skip_special_tokens=True)

        tiempo = time.time() - inicio

        # Agregar un disclaimer mÃ©dico al reporte
        disclaimer = """

âš ï¸ DISCLAIMER MÃ‰DICO:
Este reporte es generado por IA con propÃ³sito educativo y demostrativo Ãºnicamente.
NO debe utilizarse para decisiones clÃ­nicas sin validaciÃ³n por profesionales mÃ©dicos.
Proyecto acadÃ©mico - CorporaciÃ³n Universitaria Iberoamericana.
"""

        reporte_final = reporte + disclaimer

        # Generar metadatos del procesamiento
        metadata = f"""
ğŸ“Š **InformaciÃ³n de Procesamiento:**
- â±ï¸ Tiempo: {tiempo:.2f} segundos
- ğŸ¤– Modelo: Med-GEMMA 4B (Google Health AI)
- ğŸ’» GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}
- ğŸ“ Tokens generados: {len(generated_tokens)}
- ğŸ”§ Tipo anÃ¡lisis: {tipo_analisis}
"""

        # Establecer el estado de completado
        status = f"âœ… Completado exitosamente en {tiempo:.2f}s"

        return reporte_final, metadata, status

    except Exception as e:
        # Capturar y formatear cualquier error que ocurra durante el procesamiento
        error_msg = f"""
âŒ ERROR durante el anÃ¡lisis:

{str(e)}

**Posibles soluciones:**
1. Verifica que tengas GPU habilitada (Runtime > Change runtime type)
2. Reinicia el runtime (Runtime > Restart runtime)
3. Intenta con una imagen mÃ¡s pequeÃ±a
4. Si persiste, puede ser lÃ­mite de memoria - prueba cerrar otras pestaÃ±as
"""
        print(f"\nâŒ Error completo:\n{traceback.format_exc()}")
        # Retornar mensajes de error y estado
        return error_msg, "Error en procesamiento", "âŒ Error"