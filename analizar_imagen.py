import time
import traceback
import torch

def analizar_imagen(imagen, tipo_analisis, idioma, processor, model, device):
    """
    Analiza una imagen m√©dica utilizando el modelo Med-GEMMA.

    Procesa la imagen de entrada junto con un prompt basado en el tipo de an√°lisis y idioma
    seleccionados. Genera un reporte m√©dico estructurado o descriptivo.

    Args:
        imagen (PIL.Image.Image): La imagen m√©dica a analizar.
        tipo_analisis (str, optional): El tipo de an√°lisis a realizar.
            Puede ser "Descripci√≥n General", "Hallazgos Patol√≥gicos",
            "Reporte Estructurado", o "Diagn√≥stico Diferencial".
            Por defecto es "Reporte Estructurado".
        idioma (str, optional): El idioma en el que se generar√° el reporte.
            Puede ser "Espa√±ol" o "Ingl√©s". Por defecto es "Espa√±ol".

    Returns:
        tuple: Una tupla conteniendo:
            - str: El reporte m√©dico generado o un mensaje de error.
            - str: Metadatos del procesamiento (tiempo, modelo, GPU, etc.).
            - str: El estado del proceso (Completado, Error).
    """
    # Verificar si el modelo y procesador se cargaron correctamente
    if processor is None or model is None:
        return "‚ùå Error: el modelo no se carg√≥ (verifica tu token HF o la memoria)", "", "‚ùå Error"

    if imagen is None:
        return "‚ùå Por favor carga una imagen primero", "", "Error: Sin imagen"

    try:
        inicio = time.time()
        prompts = {
            "Descripci√≥n General": "Describe esta imagen m√©dica identificando las estructuras anat√≥micas visibles.",
            "Hallazgos Patol√≥gicos": "Identifica cualquier hallazgo patol√≥gico o anormal en esta imagen m√©dica.",
            "Reporte Estructurado": "Genera un reporte m√©dico estructurado con: T√âCNICA, HALLAZGOS e IMPRESI√ìN.",
            "Diagn√≥stico Diferencial": "Proporciona un diagn√≥stico diferencial basado en los hallazgos visibles."
        }
        prompt = prompts.get(tipo_analisis, prompts["Reporte Estructurado"])
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "Eres un radi√≥logo experto especializado en interpretaci√≥n de im√°genes m√©dicas."}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image", "image": imagen}
                ]
            }
        ]
        text_inputs = processor.apply_chat_template(
            messages, add_generation_prompt=True, tokenize=False
        )
        inputs = processor(
            text=text_inputs,
            images=imagen,
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        input_len = inputs["input_ids"].shape[-1]
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=350,
                do_sample=False,
                num_beams=1,
                pad_token_id=processor.tokenizer.pad_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )

        generated_tokens = outputs[0][input_len:]
        reporte = processor.decode(generated_tokens, skip_special_tokens=True)
        tiempo = time.time() - inicio
        disclaimer = """

‚ö†Ô∏è DISCLAIMER M√âDICO:
Este reporte es generado por IA solo con prop√≥sito educativo y demostrativo.
NO debe usarse para decisiones cl√≠nicas sin validaci√≥n profesional.
"""
        reporte_final = reporte + disclaimer
        metadata = f"""
üìä **Informaci√≥n de Procesamiento:**
- ‚è±Ô∏è Tiempo: {tiempo:.2f} segundos
- ü§ñ Modelo: Med-GEMMA 4B (Google Health AI)
- üíª GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}
- üìù Tokens generados: {len(generated_tokens)}
- üîß Tipo an√°lisis: {tipo_analisis}
"""
        status = f"‚úÖ Completado en {tiempo:.2f}s"
        return reporte_final, metadata, status
    except Exception as e:
        error_msg = f"""
‚ùå ERROR durante el an√°lisis:

{str(e)}

**Posibles soluciones:**
1. Verifica tu token HuggingFace y acceso al modelo.
2. Reinicia la app si persiste.
3. Intenta con una imagen m√°s peque√±a o con menos carga en el sistema.
"""
        print(f"\n‚ùå Error completo:\n{traceback.format_exc()}")
        return error_msg, "Error en procesamiento", "‚ùå Error"